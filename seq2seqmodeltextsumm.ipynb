{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN47bnY6RwFqoR5atCnIkr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akashpandey237/Akashpandey237-Data-Augmentation-in-Deep-Learning/blob/main/seq2seqmodeltextsumm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Bquuc_Vsa6r"
      },
      "outputs": [],
      "source": [
        "# Set up Kaggle API credentials (replace with your own)\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset from Kaggle\n",
        "!kaggle datasets download -d gowrishankarp/newspaper-text-summarization-cnn-dailymail\n",
        "!unzip newspaper-text-summarization-cnn-dailymail.zip\n",
        "\n",
        "# Load data into DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/cnn_dailymail/train.csv')\n",
        "test_df = pd.read_csv('/content/cnn_dailymail/test.csv')\n",
        "\n",
        "# Preprocess text data (tokenization, padding, etc.)\n",
        "# ... (Fill in with your specific preprocessing steps)"
      ],
      "metadata": {
        "id": "1jScaTIPscyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6ba53f-0ddb-4d0f-8944-a72296c9ed03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading newspaper-text-summarization-cnn-dailymail.zip to /content\n",
            " 98% 495M/503M [00:04<00:00, 127MB/s]\n",
            "100% 503M/503M [00:04<00:00, 119MB/s]\n",
            "Archive:  newspaper-text-summarization-cnn-dailymail.zip\n",
            "  inflating: cnn_dailymail/test.csv  \n",
            "  inflating: cnn_dailymail/train.csv  \n",
            "  inflating: cnn_dailymail/validation.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tLJdfjvQ-9rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding, Attention\n",
        "# ... (Add any other required layers)\n",
        "vocab_size = 10000\n",
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "# Define encoder-decoder architecture with attention\n",
        "# Example using LSTM layers (adapt for GRU if preferred)\n",
        "max_input_length = 1000  # Replace with the actual maximum length\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_input_length,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units=hidden_units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "max_output_length = 50  # Replace with the actual maximum length\n",
        "decoder_inputs = Input(shape=(max_output_length,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(units=hidden_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "# Attention layer\n",
        "attention = Attention()([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenate attention output and decoder output\n",
        "decoder_concat_output = Concatenate(axis=-1)([decoder_outputs, attention])\n",
        "\n",
        "# Dense layer for prediction\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_output)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Compile the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uqjOUxP_sqMM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "6vguxxRJ-_Oo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "fmHfSaFIs3yU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a36f1ef-3bd0-4054-e1ef-faf26603c6a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1000)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 1000, 128)            1280000   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 50, 128)              1280000   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 128),                131584    ['embedding[0][0]']           \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 50, 128),            131584    ['embedding_1[0][0]',         \n",
            "                              (None, 128),                           'lstm[0][1]',                \n",
            "                              (None, 128)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 50, 128)              0         ['lstm_1[0][0]',              \n",
            "                                                                     'lstm[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 50, 256)              0         ['lstm_1[0][0]',              \n",
            "                                                                     'attention[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 50, 10000)            2570000   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5393168 (20.57 MB)\n",
            "Trainable params: 5393168 (20.57 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Replace these with your actual data\n",
        "encoder_input_texts = [\"your\", \"encoder\", \"input\", \"texts\"]\n",
        "decoder_input_texts = [\"your\", \"decoder\", \"input\", \"texts\"]\n",
        "decoder_target_texts = [\"your\", \"decoder\", \"target\", \"texts\"]\n",
        "\n",
        "# Set your maximum sequence lengths\n",
        "max_encoder_seq_length = 1000  # Replace with the actual maximum length for encoder sequences\n",
        "max_decoder_seq_length = 50   # Replace with the actual maximum length for decoder sequences\n",
        "\n",
        "# Tokenize and pad sequences for encoder input\n",
        "encoder_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "encoder_tokenizer.fit_on_texts(encoder_input_texts)\n",
        "encoder_input_data = pad_sequences(encoder_tokenizer.texts_to_sequences(encoder_input_texts), maxlen=max_encoder_seq_length, padding='post')\n",
        "\n",
        "# Tokenize and pad sequences for decoder input and target\n",
        "decoder_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "decoder_tokenizer.fit_on_texts(decoder_input_texts + decoder_target_texts)\n",
        "decoder_input_data = pad_sequences(decoder_tokenizer.texts_to_sequences(decoder_input_texts), maxlen=max_decoder_seq_length, padding='post')\n",
        "decoder_target_data = pad_sequences(decoder_tokenizer.texts_to_sequences(decoder_target_texts), maxlen=max_decoder_seq_length, padding='post')\n",
        "\n",
        "# Convert decoder target data to one-hot encoding\n",
        "decoder_target_data = to_categorical(decoder_target_data, num_classes=vocab_size)\n",
        "\n",
        "# Train-test split\n",
        "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_target_train, decoder_target_val = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "4akZ2mLU2LrV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input_train.shape)\n",
        "print(decoder_input_train.shape)\n",
        "print(decoder_target_train.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKW7coUL3v-K",
        "outputId": "8fffb89d-a146-4637-887d-c95c1c308d7f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 1000)\n",
            "(3, 50)\n",
            "(3, 50, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 10  # Replace with the desired number of epochs\n",
        "batch_size = 32  # Replace with your preferred batch size\n",
        "validation_split = 0.2  # Replace with the desired validation split\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_train, decoder_input_train],\n",
        "    decoder_target_train,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=validation_split\n",
        ")"
      ],
      "metadata": {
        "id": "LO9jT3eVx5od",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1810e2cb-0f80-438b-b201-75bb8eb41fc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 12s 12s/step - loss: 9.2087 - accuracy: 0.0000e+00 - val_loss: 9.1818 - val_accuracy: 0.9600\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.1816 - accuracy: 0.9500 - val_loss: 9.1521 - val_accuracy: 0.9800\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.1517 - accuracy: 0.9800 - val_loss: 9.1155 - val_accuracy: 0.9800\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.1148 - accuracy: 0.9800 - val_loss: 9.0660 - val_accuracy: 0.9800\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.0648 - accuracy: 0.9800 - val_loss: 8.9932 - val_accuracy: 0.9800\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.9913 - accuracy: 0.9800 - val_loss: 8.8774 - val_accuracy: 0.9800\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.8743 - accuracy: 0.9800 - val_loss: 8.6790 - val_accuracy: 0.9800\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.6736 - accuracy: 0.9800 - val_loss: 8.3361 - val_accuracy: 0.9800\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.3264 - accuracy: 0.9800 - val_loss: 7.8593 - val_accuracy: 0.9800\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.8437 - accuracy: 0.9800 - val_loss: 7.3401 - val_accuracy: 0.9800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fbd7110e740>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate([encoder_input_val, decoder_input_val], decoder_target_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOGz4RaY1g5x",
        "outputId": "f8bc5e2a-ea99-4e59-b7e6-57dc3aa6ecfe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 657ms/step - loss: 7.3432 - accuracy: 0.9800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.343179702758789, 0.9800000190734863]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([encoder_input_val, decoder_input_val])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESie4fQKBXs1",
        "outputId": "2a06220c-1e29-4503-9d55-42d7ff512765"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"your_model_name.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY-4ePcVBcgg",
        "outputId": "940669bb-c2cc-470f-be02-1fd2882f66d8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('your_model_name.keras')\n"
      ],
      "metadata": {
        "id": "sZjl0In4Bi54"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('your_model_name.keras')\n"
      ],
      "metadata": {
        "id": "-aqL6TCUB61J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics = model.evaluate([encoder_input_val, decoder_input_val], decoder_target_val)\n",
        "print(\"Evaluation Metrics:\", evaluation_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCFxpeXSB_gH",
        "outputId": "a9b9ff8d-feee-45da-87f5-c9d9a29ed6a7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 741ms/step - loss: 7.3432 - accuracy: 0.9800\n",
            "Evaluation Metrics: [7.343179702758789, 0.9800000190734863]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZCcnrF6CfYO",
        "outputId": "4f808082-dd2d-40e5-80fb-2a9003245e94"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn_dailymail  newspaper-text-summarization-cnn-dailymail.zip  your_model_name.h5\n",
            "kaggle.json    sample_data\t\t\t\t       your_model_name.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knhBLs9fDa_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}